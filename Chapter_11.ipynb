{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11: Machine Learning\n",
    "\n",
    "Importing stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = {'svg',}\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of machine learning\n",
    "\n",
    "Distinguish *supervised* and *unsupervised* models. In the former case, the data is somehow labeled (not quite clear at the moment), in the latter, the program has to decide certain things on its own. \n",
    "\n",
    "\n",
    "### Over- and underfitting\n",
    "\n",
    "Example: Fitting a constant to a linearly increasing dataset is *underfitting* the data, fitting a 9th grade polynomial is *overfitting* it. A way to check how well the model derived from the training dataset matches new data, split a dataset into two parts: train on the first and test on the second.\n",
    "If the model itself also needs to be derived from the data, one can split the dataset into three parts: one for *training*, one for *validation* and one for *testing*.\n",
    "\n",
    "\n",
    "### Measures of correctness\n",
    "\n",
    "Consider *true positive* (tp), *false positive* (fp), *true negative* (tn) and *false negative* (fn) predictions. Based on those, there are several metrics for the predicitive powers of a model:\n",
    "\n",
    "  * **Accuracy**: fraction of correct predicitions, i.e. $a = (tp+tn)/(tp+tn+fp+fn)$ Not a good measure, since sets with very large negative fractions can lead to high accuracy even for bad models. \n",
    "  * **Precision** and **recall**: Precision measures how accurate the positive predicitons are, i.e. $p=tp/(tp+fp)$, while recall measures what fraction of the positives a model identified: $r=tp/(tp+fn)$ (i.e. the fraction of the results which are in fact positive and have been identified as positive). The *F1* score combines both measures as $F1=2*p*r/(p+r)$. Generally, the choice of a model requires a trade-off between precision and recall.\n",
    "\n",
    "\n",
    "### The bias-variance trade-off\n",
    " \n",
    "Another way of thinking about the over- and underfitting problem. **Bias** measures how bad a model performs for different sets of training data (e.g. a constant fit of a polynomial dataset). **Variance** measures the difference in performance for different data sets (e.g. the constant fit of the example would have a low variance, since it performs equally bad on every set). High bias and low variance are usually sign of underfitting.\n",
    "\n",
    "\n",
    "### Feature extraction and selection\n",
    "\n",
    "Different amounts of information can be extracted from a set of data. E.g. for constructing a spam filter, one could look whether the raw text contains the word \"Viagra\" (binary question), how often the letter \"d\" appears (number) and what the domain of the sender is (choice from a discrete set of options). It is often up to the creator of a model to decide how many features should be extracted from a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
